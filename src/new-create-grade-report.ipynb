{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "current_dateTime = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# sid = json.load(open(\"/autograder/submission/SID.json\", \"r\"))[\n",
    "#     \"SID\"\n",
    "# ]  # Needed for Gradescope\n",
    "\n",
    "# sys.path.append(\"${0%/*}\")\n",
    "\n",
    "sid = \"A17531851\"\n",
    "\n",
    "df = pd.read_csv(\"../data/grades_for_grade_report.csv\").set_index(\"SID\").loc[sid]\n",
    "\n",
    "stream = open(\"../configs.yaml\", \"r\")\n",
    "dictionary = yaml.safe_load(stream)\n",
    "\n",
    "# Data Loading Fields\n",
    "GRADES_FILENAME = dictionary[\"data_path\"][\"grades_filename\"]\n",
    "\n",
    "# Lab Fields\n",
    "NUM_LABS = dictionary[\"labs\"][\"num_labs\"]\n",
    "MAX_LABS = dictionary[\"labs\"][\"max_labs\"]\n",
    "\n",
    "# Project Fields\n",
    "NUM_PROJECTS = dictionary[\"projects\"][\"num_projects\"]\n",
    "MAX_PROJECTS = dictionary[\"projects\"][\"max_projects\"]\n",
    "\n",
    "NUM_PROJECT_CHECKPOINTS = dictionary[\"projects\"][\"num_checkpoints\"]\n",
    "MAX_PROJECT_CHECKPOINTS = dictionary[\"projects\"][\"max_checkpoints\"]\n",
    "\n",
    "# Midterm fields\n",
    "YES_MIDTERM = dictionary[\"exams\"][\"midterm\"][\"enabled\"]\n",
    "MIDTERM_VERSIONS = dictionary[\"exams\"][\"midterm\"][\"versions\"]\n",
    "MIDTERM_BONUS = dictionary[\"exams\"][\"midterm\"][\"bonus\"]\n",
    "MIDTERM_HAS_BONUS = MIDTERM_BONUS > 0\n",
    "\n",
    "# Final Fields\n",
    "YES_FINAL = dictionary[\"exams\"][\"final\"][\"enabled\"]\n",
    "FINAL_VERSIONS = dictionary[\"exams\"][\"final\"][\"versions\"]\n",
    "FINAL_BONUS = dictionary[\"exams\"][\"final\"][\"bonus\"]\n",
    "FINAL_HAS_BONUS = FINAL_BONUS > 0\n",
    "\n",
    "# Discussion Fields\n",
    "NUM_DISC_ATTENDENCE_REQUIRED = dictionary[\"discussions\"][\n",
    "    \"num_discussions_attendence_required\"\n",
    "]\n",
    "NUM_LECT_ATTENDENCE_REQUIRED = dictionary[\"discussions\"][\n",
    "    \"num_lecture_attendence_required\"\n",
    "]\n",
    "\n",
    "# Number of dropped assignments per category (set to 0 for no drops)\n",
    "# For example, if NUM_DROPS = 2, then 2 Labs and 2 Homeworks will be dropped\n",
    "# Note: Set this to 0 until the end of the quarter\n",
    "NUM_DROPS = dictionary[\"drop_policy\"][\"num_drops\"]\n",
    "\n",
    "OVERALL_EC = dictionary[\"extra_credit\"][\"overall\"]\n",
    "\n",
    "DROPS = NUM_DROPS > 0\n",
    "\n",
    "CURRENT_TOTAL_POSSIBLE_SCORE = df[\"Current Max Possible Score\"].max()\n",
    "\n",
    "ASSIGNMENT_WEIGHTS = dictionary[\"assignment_weights\"]\n",
    "DISCUSSION_ASSIGNMENT_WEIGHTS = dictionary[\"discussion_assignment_weights\"]\n",
    "\n",
    "DATE = current_dateTime\n",
    "\n",
    "DECIMAL_ROUND_PLACE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_round_str(n, round_place=DECIMAL_ROUND_PLACE):\n",
    "    rounded = str(np.round(n, round_place))\n",
    "    if len(rounded) < round_place + 2:\n",
    "        return str(rounded) + \"0\" * (round_place + 2 - len(rounded))\n",
    "    else:\n",
    "        return str(rounded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_letter_grade(df, gs_output):\n",
    "    output = [f\"{df['Name']}\\n{df['Email']}\\n{sid}\\nlast updated on {DATE}\\n\"]\n",
    "    output = np.append(\n",
    "        output,\n",
    "        [\n",
    "            f\"Your overall score in the course is {np.round(df['Overall Score'], 3)}%.\\nNote that grades **will not** be rounded.\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        [\n",
    "            f\"Your current percentage based on your scores so far is {np.round(df['Overall Score'], DECIMAL_ROUND_PLACE)} / {np.round(CURRENT_TOTAL_POSSIBLE_SCORE, DECIMAL_ROUND_PLACE)} * 100 = {np.round(df['Current Score'], DECIMAL_ROUND_PLACE)}%.\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if not DROPS:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            [\n",
    "                f\"Also note that this grade report accounts for dropped assignments but no for extra credit yet.\"\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            [\n",
    "                \"This accounts for dropped assignments – the lowest lab score was dropped\"\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Overall Score\",\n",
    "            \"score\": 0,\n",
    "            \"max_score\": 0,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_assignment_string(assignment):\n",
    "    kind = assignment.split(\" \")[0]\n",
    "    exists = df.loc[assignment + \" - Max Points\"] != 0\n",
    "    if exists:\n",
    "        output = f'{assignment}: {even_round_str(df.loc[assignment + \" Final Grade\"])}'\n",
    "        output += f' ({df.loc[assignment]} / {df.loc[assignment + \" - Max Points\"]})'\n",
    "        if DROPS and \"Lab\" in assignment:\n",
    "            if assignment in df[f\"Dropped {kind}s\"]:\n",
    "                output += \" [Dropped]\"\n",
    "        # if df[assignment + ' Slip Days'] > 0:\n",
    "        #     if assignment in df['Accepted Late Assignments']:\n",
    "        #         output += f' [{int(df[assignment + \" Slip Days\"])} slip day(s) used]'\n",
    "    else:\n",
    "        output = f\"{assignment}: 0 (Not yet released or graded)\"\n",
    "        # if DROPS and 'Lab' in assignment:\n",
    "        #     if assignment in df[f'Dropped {kind}s']:\n",
    "        #         output += ' [Dropped]'\n",
    "\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lab_grades(df, gs_output):\n",
    "    lab_score = df[\"Lab Average\"]\n",
    "    lab_weight = int(ASSIGNMENT_WEIGHTS[\"lab\"] * 100)\n",
    "    output = [\n",
    "        f\"Labs are worth {lab_weight}% of your grade. There are {MAX_LABS} labs total.\\n\"\n",
    "    ]\n",
    "    for lab in range(1, MAX_LABS + 1):\n",
    "        if lab <= NUM_LABS:\n",
    "            output = np.append(output, make_assignment_string(f\"Lab {lab}\"))\n",
    "        else:\n",
    "            output = np.append(output, f\"Lab {lab}: 0 (not yet released or graded)\")\n",
    "    output = np.append(output, [f\"Overall: {even_round_str(df.loc['Lab Average'])}\"])\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Labs\",\n",
    "            \"score\": float(lab_score) * lab_weight,\n",
    "            \"max_score\": lab_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_project_grades(df, gs_output):\n",
    "    project_score = df.loc[\"Project Average\"]\n",
    "    project_weight = int(ASSIGNMENT_WEIGHTS[\"project\"] * 100)\n",
    "    each_project_weight = int(project_weight / 5)\n",
    "    output = [\n",
    "        f\"Projects are worth {project_weight}% of your grade. There are {MAX_PROJECTS} projects. There are no drops. \\nProjects 1, 2, and 3 are each worth {each_project_weight}% of your overall grade, and Project 4 is worth {2*each_project_weight}%, for a total of {project_weight}%.\\n\"\n",
    "    ]\n",
    "    for project in range(1, MAX_PROJECTS + 1):\n",
    "        # if project == 4:\n",
    "        #     project_str = '4A'\n",
    "        # elif project == 5:\n",
    "        #     project_str = '4B'\n",
    "        # else:\n",
    "        project_str = str(project)\n",
    "        if project <= NUM_PROJECTS:\n",
    "\n",
    "            output = np.append(output, make_assignment_string(f\"Project {project_str}\"))\n",
    "        else:\n",
    "            output = np.append(\n",
    "                output, f\"Project {project_str}: 0 (not yet released or graded)\"\n",
    "            )\n",
    "\n",
    "    output = np.append(\n",
    "        output, [f\"Overall: {even_round_str(df.loc['Project Average'])}\"]\n",
    "    )\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Projects\",\n",
    "            \"score\": float(project_score) * project_weight,\n",
    "            \"max_score\": project_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_discussion(df, gs_output):\n",
    "    if df['Used Discussion']:\n",
    "        discussion_weight = int(DISCUSSION_ASSIGNMENT_WEIGHTS[\"discussion\"] * 100)\n",
    "    else:\n",
    "        discussion_weight = 0\n",
    "    discussion_attended = df['disc_count']\n",
    "    lecture_attended = df['lecture_count']\n",
    "\n",
    "    output = [\n",
    "        f'Student who attended {NUM_DISC_ATTENDENCE_REQUIRED} discussions and {NUM_LECT_ATTENDENCE_REQUIRED} lectures will receive full discussion credit, worth 5% of your overall grade.\\nIf you attend less discussion or lecture, discussion will be worth 0%, and your midterm and final exam grade will each be worth 2.5% more.'\n",
    "    ]\n",
    "    output = np.append(output, f'You attended {discussion_attended} discussions and {lecture_attended} lectures.')\n",
    "    if df['Elgible for Discussion']:\n",
    "        output = np.append(output, f'You are elgible for discussion credit.')\n",
    "        output = np.append(output, f'Your grade with discussion is {even_round_str(df[\"Overall Score with Discussion\"])}')\n",
    "        output = np.append(output, f'Your grade without discussion is {even_round_str(df[\"Overall Score without Discussion\"])}')\n",
    "        if df['Used Discussion']:\n",
    "            output = np.append(output, f'Your score improved with discussion.')\n",
    "        else:\n",
    "            output = np.append(output, f'Your score did not improve with discussion.')\n",
    "    else:\n",
    "        output = np.append(output, f'You are not elgible for discussion credit.')\n",
    "    \n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Discussion\",\n",
    "            \"score\": float(discussion_weight),\n",
    "            \"max_score\": discussion_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_project_checkpoints(df, gs_output):\n",
    "    checkpoint_score = df.loc[\"Project Checkpoint Average\"]\n",
    "    checkpoint_weight = int(ASSIGNMENT_WEIGHTS[\"project_checkpoint\"] * 100)\n",
    "    output = [\n",
    "        f\"Project Checkpoints are worth {checkpoint_weight}% of your grade. There are {MAX_PROJECT_CHECKPOINTS} project checkpoints. There are no drops. \\nEach checkpoints are each worth 1% of your overall grade.\\n\"\n",
    "    ]\n",
    "    for checkpoint in range(1, MAX_PROJECT_CHECKPOINTS + 1):\n",
    "        # if checkpoint == 4:\n",
    "        #     checkpoint_str = '4.1'\n",
    "        # elif checkpoint == 5:\n",
    "        #     checkpoint_str = '4.2'\n",
    "        # else:\n",
    "        checkpoint_str = str(checkpoint)\n",
    "        if checkpoint <= NUM_PROJECT_CHECKPOINTS:\n",
    "            output = np.append(\n",
    "                output, make_assignment_string(f\"Project {checkpoint_str} Checkpoint\")\n",
    "            )\n",
    "        else:\n",
    "            output = np.append(\n",
    "                output, f\"Project {checkpoint_str}: 0 (not yet released or graded)\"\n",
    "            )\n",
    "\n",
    "    output = np.append(\n",
    "        output, [f\"Overall: {even_round_str(df.loc['Project Checkpoint Average'])}\"]\n",
    "    )\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Project Checkpoints\",\n",
    "            \"score\": float(checkpoint_score) * checkpoint_weight,\n",
    "            \"max_score\": checkpoint_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_midterm_exam_grade(df, gs_output):\n",
    "    if df[\"Used Discussion\"]:\n",
    "        midterm_weight = int(DISCUSSION_ASSIGNMENT_WEIGHTS[\"midterm_exam\"] * 100)\n",
    "    else:\n",
    "        midterm_weight = int(ASSIGNMENT_WEIGHTS[\"midterm_exam\"] * 100)\n",
    "    midterm_score = df.loc[\"Midterm Average\"]\n",
    "\n",
    "    output = [f\"The Midterm Exam is worth {midterm_weight}% of your grade.\"]\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Pre-Redemption Score: {even_round_str(df.loc[\"Midterm Exam Grade\"])} ({df.loc[\"Midterm\"]} / {df.loc[\"Midterm - Max Points\"]})',\n",
    "    )\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Mean: {even_round_str(df.loc[\"Midterm Mean\"])}; Midterm std: {even_round_str(df.loc[\"Midterm std\"])}',\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm z-score: {even_round_str(df.loc[\"Midterm Z-Score\"])} = ({even_round_str(df.loc[\"Midterm Exam Grade\"])} - {even_round_str(df.loc[\"Midterm Mean\"])}) / {even_round_str(df.loc[\"Midterm std\"])}',\n",
    "    )\n",
    "    # if df['Midterm Clobbered']:\n",
    "    #     output = np.append(output, f\"Since you made a special arrangement to replace your Midterm Exam score with your Final Exam score, the score you see above is the same in standard units as your Final Exam score.\")\n",
    "\n",
    "    output = np.append(\n",
    "        output, f'The redemption questions on the final exam are marked with \"(M)\".'\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Redemption Mean: {even_round_str(df.loc[\"Midterm Redemption Mean\"])}; Midterm Redemption std: {even_round_str(df.loc[\"Midterm Redemption std\"])}',\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Redemption z-score: {even_round_str(df.loc[\"Midterm Redemption Z-Score\"])} = ({even_round_str(df.loc[\"Redemption Score\"])} - {even_round_str(df.loc[\"Midterm Redemption Mean\"])}) / {even_round_str(df.loc[\"Midterm Redemption std\"])}',\n",
    "    )\n",
    "\n",
    "    if df[\"Redemption Successful\"]:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            f\"Redemption Successful: new midterm score is {even_round_str(midterm_score)} = {even_round_str(df['Midterm Redemption Z-Score'])} * {even_round_str(df.loc['Midterm std'])} + {even_round_str(df.loc['Midterm Mean'])}.\",\n",
    "        )\n",
    "    else:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            f\"Redemption Unsuccessful: your new z-score is not higher than your old z-score.\",\n",
    "        )\n",
    "\n",
    "    output = np.append(output, f\"Midterm Final Score: {even_round_str(midterm_score)}\")\n",
    "\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Midterm Exam\",\n",
    "            \"score\": float(midterm_score * midterm_weight),\n",
    "            \"max_score\": midterm_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_final_exam_grade(df, gs_output):\n",
    "    if df[\"Used Discussion\"]:\n",
    "        final_exam_weight = int(DISCUSSION_ASSIGNMENT_WEIGHTS[\"final_exam\"] * 100)\n",
    "    else:\n",
    "        final_exam_weight = int(ASSIGNMENT_WEIGHTS[\"final_exam\"] * 100)\n",
    "    final_exam_score = df.loc[\"Final Average\"]\n",
    "    output = [f\"The Final Exam is worth {final_exam_weight}% of your grade.\"]\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Score: {even_round_str(final_exam_score)} ({df.loc[\"Final Exam\"]} / {df.loc[\"Final Exam - Max Points\"]})',\n",
    "    )\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Final Exam\",\n",
    "            \"score\": float(final_exam_score * final_exam_weight),\n",
    "            \"max_score\": final_exam_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_output = {\"tests\": []}\n",
    "output_letter_grade(df, gs_output)\n",
    "output_discussion(df, gs_output)\n",
    "if NUM_LABS > 0:\n",
    "    output_lab_grades(df, gs_output)\n",
    "if NUM_PROJECTS > 0:\n",
    "    output_project_grades(df, gs_output)\n",
    "    output_project_checkpoints(df, gs_output)\n",
    "\n",
    "if YES_MIDTERM:\n",
    "    output_midterm_exam_grade(df, gs_output)\n",
    "\n",
    "if YES_FINAL:\n",
    "    output_final_exam_grade(df, gs_output)\n",
    "\n",
    "\n",
    "# out_path = \"/autograder/results/results.json\"\n",
    "# with open(out_path, \"w\") as f:\n",
    "#     f.write(json.dumps(gs_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdulrahim Ham\n",
      "amham@ucsd.edu\n",
      "A17531851\n",
      "last updated on 2024-06-17\n",
      "\n",
      "Your overall score in the course is 79.659%.\n",
      "Note that grades **will not** be rounded.\n",
      "Your current percentage based on your scores so far is 79.659 / 100.0 * 100 = 79.659%.\n",
      "This accounts for dropped assignments – the lowest lab score was dropped\n",
      "\n",
      "Student who attended 8 discussions and 16 lectures will receive full discussion credit, worth 5% of your overall grade.\n",
      "If you attend less discussion or lecture, discussion will be worth 0%, and your midterm and final exam grade will each be worth 2.5% more.\n",
      "You attended 1 discussions and 16 lectures.\n",
      "You are not elgible for discussion credit.\n",
      "\n",
      "Labs are worth 20% of your grade. There are 9 labs total.\n",
      "\n",
      "Lab 1: 0.9699 (129.0 / 133.0)\n",
      "Lab 2: 0.9245 (61.5 / 79.5)\n",
      "Lab 3: 0.9286 (52.0 / 56.0)\n",
      "Lab 4: 0.9624 (89.5 / 93.0)\n",
      "Lab 5: 0.9918 (60.5 / 61.0)\n",
      "Lab 6: 0.9107 (51.0 / 56.0)\n",
      "Lab 7: 0.9730 (108.0 / 111.0)\n",
      "Lab 8: nan000 (nan / 65.0) [Dropped]\n",
      "Lab 9: 0.9636 (53.0 / 55.0)\n",
      "Overall: 0.9531\n",
      "\n",
      "Projects are worth 25% of your grade. There are 4 projects. There are no drops. \n",
      "Projects 1, 2, and 3 are each worth 5% of your overall grade, and Project 4 is worth 10%, for a total of 25%.\n",
      "\n",
      "Project 1: 0.9268 (114.0 / 123.0)\n",
      "Project 2: 0.9109 (92.0 / 101.0)\n",
      "Project 3: 0.7900 (79.0 / 100.0)\n",
      "Project 4: 0.9350 (187.0 / 200.0)\n",
      "Overall: 0.8995\n",
      "\n",
      "Project Checkpoints are worth 5% of your grade. There are 5 project checkpoints. There are no drops. \n",
      "Each checkpoints are each worth 1% of your overall grade.\n",
      "\n",
      "Project 1 Checkpoint: 1.0000 (13.0 / 13.0)\n",
      "Project 2 Checkpoint: 1.0000 (5.0 / 5.0)\n",
      "Project 3 Checkpoint: 1.0000 (10.0 / 10.0)\n",
      "Project 4 Checkpoint: 1.0000 (20.0 / 20.0)\n",
      "Project 5 Checkpoint: 1.0000 (20.0 / 20.0)\n",
      "Overall: 1.0000\n",
      "\n",
      "The Midterm Exam is worth 20% of your grade.\n",
      "Midterm Pre-Redemption Score: 0.3600 (18.0 / 50.0)\n",
      "Midterm Mean: 0.5002; Midterm std: 0.1552\n",
      "Midterm z-score: -0.9035 = (0.3600 - 0.5002) / 0.1552\n",
      "The redemption questions on the final exam are marked with \"(M)\".\n",
      "Midterm Redemption Mean: 19.9027; Midterm Redemption std: 6.7478\n",
      "Midterm Redemption z-score: 0.5331 = (23.500 - 19.9027) / 6.7478\n",
      "Redemption Successful: new midterm score is 0.5830 = 0.5331 * 0.1552 + 0.5002.\n",
      "Midterm Final Score: 0.5830\n",
      "\n",
      "The Final Exam is worth 30% of your grade.\n",
      "Score: 0.7150 (71.5 / 100.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dict in gs_output[\"tests\"]:\n",
    "    print(dict[\"output\"]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
