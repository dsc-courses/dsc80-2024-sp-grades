{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "current_dateTime = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# metadata = json.load(open('/autograder/submission_metadata.json', 'r')) # Needed for Gradescope\n",
    "# import os\n",
    "\n",
    "# files = os.listdir('/autograder/submission')\n",
    "# for file in files:\n",
    "#     print(file)\n",
    "# sid =  json.load(open('/autograder/submission/SID.json', 'r'))[\"SID\"] # Needed for Gradescope\n",
    "sid = 'A17531851'\n",
    "\n",
    "\n",
    "# sys.path.append(\"${0%/*}\")\n",
    "# subprocess.run('ls')\n",
    "\n",
    "df = pd.read_csv('../data/grades_for_grade_report.csv').set_index('SID').loc[sid]\n",
    "\n",
    "stream = open(\"../configs.yaml\", 'r')\n",
    "dictionary = yaml.safe_load(stream)\n",
    "\n",
    "# Data Loading Fields\n",
    "GRADES_FILENAME = dictionary['data_path']['grades_filename']\n",
    "# ROSTER_FILENAME = 'sheets/dsc10-sp23-roster-final.csv'\n",
    "# ATTENDANCE_PATH = 'sheets/discussions'\n",
    "\n",
    "# Lab Fields\n",
    "NUM_LABS = dictionary['labs']['num_labs']\n",
    "MAX_LABS = dictionary['labs']['max_labs']\n",
    "\n",
    "# Project Fields\n",
    "NUM_PROJECTS = dictionary['projects']['num_projects']\n",
    "MAX_PROJECTS = dictionary['projects']['max_projects']\n",
    "\n",
    "NUM_PROJECT_CHECKPOINTS = dictionary['projects']['num_checkpoints']\n",
    "MAX_PROJECT_CHECKPOINTS = dictionary['projects']['max_checkpoints']\n",
    "\n",
    "\n",
    "# Midterm fields\n",
    "YES_MIDTERM = dictionary['exams']['midterm']['enabled']\n",
    "MIDTERM_VERSIONS = dictionary['exams']['midterm']['versions']\n",
    "MIDTERM_BONUS = dictionary['exams']['midterm']['bonus']\n",
    "MIDTERM_HAS_BONUS = MIDTERM_BONUS > 0\n",
    "\n",
    "# Final Fields\n",
    "YES_FINAL = dictionary['exams']['final']['enabled']\n",
    "FINAL_VERSIONS = dictionary['exams']['final']['versions']\n",
    "FINAL_BONUS = dictionary['exams']['final']['bonus']\n",
    "FINAL_HAS_BONUS = FINAL_BONUS > 0\n",
    "\n",
    "# Discussion Fields\n",
    "NUM_DISC_AND_LECT_ATTENDENCE_REQUIRED = dictionary[\"discussions\"][\"num_discussions_and_lecture_attendence_required\"]\n",
    "\n",
    "# Number of dropped assignments per category (set to 0 for no drops)\n",
    "# For example, if NUM_DROPS = 2, then 2 Labs and 2 Homeworks will be dropped\n",
    "# Note: Set this to 0 until the end of the quarter\n",
    "NUM_DROPS = dictionary['drop_policy']['num_drops']\n",
    "\n",
    "OVERALL_EC = dictionary['extra_credit']['overall']\n",
    "\n",
    "DROPS = NUM_DROPS > 0\n",
    "\n",
    "CURRENT_TOTAL_POSSIBLE_SCORE = df['Current Max Possible Score'].max()\n",
    "\n",
    "ASSIGNMENT_WEIGHTS = dictionary[\"assignment_weights\"]\n",
    "DISCUSSION_ASSIGNMENT_WEIGHTS = dictionary[\"discussion_assignment_weights\"]\n",
    "\n",
    "DATE = current_dateTime\n",
    "\n",
    "DECIMAL_ROUND_PLACE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_round_str(n, round_place=DECIMAL_ROUND_PLACE):\n",
    "    rounded = str(np.round(n, round_place))\n",
    "    if len(rounded) < round_place + 2:\n",
    "        return str(rounded) + \"0\" * (round_place + 2 - len(rounded))\n",
    "    else:\n",
    "        return str(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_letter_grade(df, gs_output):\n",
    "    output = [f\"{df['Name']}\\n{df['Email']}\\n{sid}\\nlast updated on {DATE}\\n\"]\n",
    "    output = np.append(output, [f\"Your overall score in the course is {np.round(df['Overall Score'], 3)}%.\\nNote that grades **will not** be rounded.\"])\n",
    "    \n",
    "    # if df[\"Grade Option\"] == \"P\":\n",
    "    #     output = np.append(output, f\"Letter Grade: {df['Letter Grade']} (but since you're taking the class P/NP, we will submit {df['Final_Assigned_Egrade']})\")\n",
    "    # else:\n",
    "    #     output = np.append(output, f\"Letter Grade: {df['Letter Grade']}\")\n",
    "    \n",
    "    output = np.append(output, [f\"Your current percentage based on your scores so far is {np.round(df['Overall Score'], DECIMAL_ROUND_PLACE)} / {np.round(CURRENT_TOTAL_POSSIBLE_SCORE, DECIMAL_ROUND_PLACE)} * 100 = {np.round(df['Current Score'], DECIMAL_ROUND_PLACE)}%.\"])\n",
    "    # output = np.append(output, [f\"Your maximum possible score in the course is {max_pos}% if you get perfect scores on all remaining assignments (not including extra credit from discussion).\"])\n",
    "    \n",
    "    if not DROPS:\n",
    "        output = np.append(output, [f\"Also note that this grade report accounts for dropped assignments but no for extra credit yet.\"])\n",
    "    else:\n",
    "        output = np.append(output, [\"This accounts for dropped assignments – the lowest lab score was dropped\"])\n",
    "        # – and all possible extra credit (discussion attendance, midterm SETs bonus, final SETs bonus, and Lab 9).\n",
    "\n",
    "\n",
    "\n",
    "    # output = np.append(output, [\"The Discussion 10 Extra Credit Opportunities are not included in this grade report yet.\"])\n",
    "\n",
    "    #output = np.append(output, [f\"Your overall score in the course is {np.round(df['Overall Score'], 4)}%.\\nLetter Grade: {df['Letter Grade']}\"])\n",
    "    # output = np.append(output, [f\"\\nLetter Grade: {df['Letter Grade']}\"])\n",
    "\n",
    "    gs_output['tests'].append({\n",
    "        'name': 'Overall Score',\n",
    "        'score': 0,\n",
    "        'max_score': 0,\n",
    "        'output': '\\n'.join(output),\n",
    "        'visibility': 'after_published'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_assignment_string(assignment):\n",
    "    kind = assignment.split(' ')[0]\n",
    "    exists = df.loc[assignment + ' - Max Points'] != 0\n",
    "    if exists:\n",
    "        output = f'{assignment}: {even_round_str(df.loc[assignment + \" Final Grade\"])}'\n",
    "        output += f' ({df.loc[assignment]} / {df.loc[assignment + \" - Max Points\"]})'\n",
    "        if DROPS and 'Lab' in assignment:\n",
    "            if assignment in df[f'Dropped {kind}s']:\n",
    "                output += ' [Dropped]'\n",
    "        # if df[assignment + ' Slip Days'] > 0:\n",
    "        #     if assignment in df['Accepted Late Assignments']:\n",
    "        #         output += f' [{int(df[assignment + \" Slip Days\"])} slip day(s) used]'\n",
    "    else:\n",
    "        output = f'{assignment}: 0 (Not yet released or graded)'\n",
    "        # if DROPS and 'Lab' in assignment:\n",
    "        #     if assignment in df[f'Dropped {kind}s']:\n",
    "        #         output += ' [Dropped]'\n",
    "            \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lab_grades(df, gs_output):\n",
    "    lab_score = df['Lab Average']\n",
    "    output = [f'Labs are worth 20% of your grade. There are {MAX_LABS} labs total.\\n']\n",
    "    for lab in range(1, MAX_LABS+1):\n",
    "        if lab <= NUM_LABS:\n",
    "            output = np.append(output, make_assignment_string(f'Lab {lab}'))\n",
    "        else:\n",
    "            output = np.append(output, f'Lab {lab}: 0 (not yet released or graded)')\n",
    "    output = np.append(output, [f\"Overall: {even_round_str(df.loc['Lab Average'])}\"])\n",
    "    gs_output['tests'].append({\n",
    "        'name': 'Labs',\n",
    "        'score': float(lab_score) * 20,\n",
    "        'max_score': 20,\n",
    "        'output': '\\n'.join(output),\n",
    "        'visibility': 'after_published'\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_project_grades(df, gs_output):\n",
    "    project_score = df.loc['Project Average']\n",
    "    output = [f'Projects are worth 25% of your grade. There are {MAX_PROJECTS} projects. There are no drops. \\nProjects 1, 2, and 3 are each worth 6% of your overall grade, and Project 4 is worth 12%, for a total of 30%.\\n']\n",
    "    for project in range(1, MAX_PROJECTS+1):\n",
    "        # if project == 4:\n",
    "        #     project_str = '4A'\n",
    "        # elif project == 5:\n",
    "        #     project_str = '4B'\n",
    "        # else:\n",
    "        project_str = str(project)\n",
    "        if project <= NUM_PROJECTS:\n",
    "        \n",
    "            output = np.append(output, make_assignment_string(f'Project {project_str}'))\n",
    "        else:\n",
    "            output = np.append(output, f'Project {project_str}: 0 (not yet released or graded)')\n",
    "\n",
    "    output = np.append(output, [f\"Overall: {even_round_str(df.loc['Project Average'])}\"])\n",
    "    gs_output['tests'].append({\n",
    "        'name': 'Projects',\n",
    "        'score': float(project_score) * 25,\n",
    "        'max_score': 25,\n",
    "        'output': '\\n'.join(output),\n",
    "        'visibility': 'after_published'\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_discussion_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_project_checkpoints(df, gs_output):    \n",
    "    checkpoint_score = df.loc['Project Checkpoint Average']\n",
    "    output = [f'Project Checkpoints are worth 5% of your grade. There are {MAX_PROJECT_CHECKPOINTS} project checkpoints. There are no drops. \\nThe checkpoints for Projects 1, 2, and 3 are each worth 1% of your overall grade, and the checkpoint for Project 4 is worth 2%, for a total of 5%.\\n']\n",
    "    for checkpoint in range(1, MAX_PROJECT_CHECKPOINTS+1):\n",
    "        # if checkpoint == 4:\n",
    "        #     checkpoint_str = '4A'\n",
    "        # elif checkpoint == 5:\n",
    "        #     checkpoint_str = '4B'\n",
    "        # else:\n",
    "        checkpoint_str = str(checkpoint)\n",
    "        if checkpoint <= NUM_PROJECT_CHECKPOINTS:\n",
    "            output = np.append(output, make_assignment_string(f'Project {checkpoint_str} Checkpoint'))\n",
    "        else:\n",
    "            output = np.append(output, f'Project {checkpoint_str}: 0 (not yet released or graded)')\n",
    "\n",
    "    output = np.append(output, [f\"Overall: {even_round_str(df.loc['Project Checkpoint Average'])}\"])\n",
    "    gs_output['tests'].append({\n",
    "        'name': 'Project Checkpoints',\n",
    "        'score': float(checkpoint_score) * 5,\n",
    "        'max_score': 5,\n",
    "        'output': '\\n'.join(output),\n",
    "        'visibility': 'after_published'\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_midterm_exam_grade(df, gs_output):    \n",
    "    # output = ['The Midterm Exam is worth 15% of your grade.']\n",
    "    # output = np.append(output, f'Score: {even_round_str(df.loc[\"Midterm Exam Raw\"])} ({df.loc[\"Midterm Exam\"]} / {df.loc[\"Midterm Exam - Max Points\"]} + {MIDTERM_BONUS} bonus points)')\n",
    "    # # if df['Midterm Clobbered']:\n",
    "    # #     output = np.append(output, f\"Since you made a special arrangement to replace your Midterm Exam score with your Final Exam score, the score you see above is the same in standard units as your Final Exam score.\")\n",
    "\n",
    "    # output = np.append(output, f\"The redemption questions on the final exam are marked with \\\"(R)\\\".\")\n",
    "\n",
    "    # output = np.append(output, f'Midterm Exam z-score: {even_round_str(df.loc[\"Midterm z-score\"])}; Redemption z-score: {even_round_str(df.loc[\"Midterm Redemption Part z-score\"])}; Midterm std: {even_round_str(df.loc[\"Midterm std\"])}; Midterm mean: {even_round_str(df.loc[\"Midterm Mean\"])}')\n",
    "\n",
    "    # if df['Redemption Successful']:\n",
    "    #     output = np.append(output, f\"Redemption Successful: new midterm score is {even_round_str(df['Midterm Final Raw'])} = {even_round_str(df['Midterm Redemption Part z-score'])} * {even_round_str(df.loc['Midterm std'])} + {even_round_str(df.loc['Midterm Mean'])}.\")\n",
    "    # else:\n",
    "    #     output = np.append(output, f\"Redemption Unsuccessful: your new z-score is not higher than your old z-score.\")\n",
    "\n",
    "    # output = np.append(output, f'Midterm Final Score: {even_round_str(df.loc[\"Midterm Final Raw\"])}')\n",
    "\n",
    "    # if YES_FINAL:\n",
    "    #     C=even_round_str(df.loc[\"Redemption Score\"]), \n",
    "    #     D=even_round_str(df.loc[\"Midterm Redemption Part z-score\"]), \n",
    "\n",
    "    #     output = f\"\"\"The Midterm Exam is worth 20% of your grade.\n",
    "    # - Your score on the original Midterm Exam, without the 2 point (2.5%) bonus we gave everyone on the Midterm Exam, was {even_round_str(df.loc[\"Midterm Exam Grade Pre-EC\"])}. \n",
    "    #     Since the original Midterm Exam had a mean of {even_round_str(df.loc[\"Midterm Mean\"])} and SD of {even_round_str(df.loc[\"Midterm std\"])},\n",
    "    #     your score as a z-score is {even_round_str(df.loc[\"Midterm z-score\"])}.\n",
    "    # - Your score on the redemption questions on the Final Exam (Questions 1-n) was {even_round_str(df.loc[\"Redemption Score\"])}. \n",
    "    #     Converted to a z-score, this is {even_round_str(df.loc[\"Midterm Redemption Part z-score\"])}.\"\"\"\n",
    "\n",
    "    #     if df.loc[\"Midterm Redemption Part z-score\"] > df.loc[\"Midterm z-score\"]:\n",
    "    #         output += f'''\n",
    "\n",
    "    # Congrats! Since your redemption z-score is higher than your original z-score, your Midterm Exam score will increase.\n",
    "    # Specifically, it will increase from {even_round_str(df.loc[\"Midterm Exam Raw Pre-EC\"])} to {even_round_str(df.loc[\"Midterm Mean\"])} + ({even_round_str(df.loc[\"Midterm Redemption Part z-score\"])}) * {even_round_str(df.loc[\"Midterm std\"])} = {even_round_str(df.loc[\"Midterm Exam Raw Post-Redemption Pre-EC\"])}.\n",
    "    # '''\n",
    "            \n",
    "    #     else:\n",
    "    #         output += f'''\n",
    "\n",
    "    # Since your redemption z-score is lower than your original z-score, your Midterm Exam score will not change.\n",
    "    # '''\n",
    "        \n",
    "    #     output += f'''\n",
    "    # Including the 2 point (2.5% bonus), your Midterm Exam score will be entered as {even_round_str(df.loc[\"Midterm Exam Raw Post-Redemption Pre-EC\"])} + 0.025 = {even_round_str(df.loc[\"Midterm Exam Raw Post-Redemption Post-EC\"])}.\n",
    "    # '''\n",
    "    # else:\n",
    "    output = f\"\"\"The Midterm Exam is worth 20% of your grade.\n",
    "- Your score on the original Midterm Exam was {even_round_str(df.loc[\"Midterm Exam Grade Pre-EC\"])}. \n",
    "    Since the original Midterm Exam had a mean of {even_round_str(df.loc[\"Midterm Mean\"])} and SD of {even_round_str(df.loc[\"Midterm std\"])},\n",
    "    your score as a z-score is {even_round_str(df.loc[\"Midterm Z-Score\"])}.\n",
    "- On the Final there will be questions to Redeem your Midterm Exam score. \n",
    "    If your Z-Score higher on these questions than your original Midterm Exam score, your Midterm Exam score will increase.\n",
    "    If you score lower, your Midterm Exam score will not change.\"\"\"\n",
    "\n",
    "#.format(\n",
    "#     A=, \n",
    "#     B=, \n",
    "#     C=, \n",
    "#     D=, \n",
    "#     E=even_round_str(df.loc[\"Midterm Exam Raw Post-Redemption Pre-EC\"]), \n",
    "#     F=even_round_str(df.loc[\"Midterm Exam Raw Post-Redemption Post-EC\"]),\n",
    "#     mean=even_round_str(df.loc['Midterm Mean']),\n",
    "#     sd=even_round_str(df.loc['Midterm std'])\n",
    "    gs_output['tests'].append({\n",
    "        'name': 'Midterm Exam',\n",
    "        'score': float(df['Midterm Exam Grade Post-Redemption Post-EC'] * 15),\n",
    "        'max_score': 20,\n",
    "        'output': output, #'\\n'.join(output),\n",
    "        'visibility': 'after_published'\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_final_exam_grade(df, gs_output):  \n",
    "    output = ['The Final Exam is worth 30% of your grade.']\n",
    "    output = np.append(output, f'Score: {even_round_str(df.loc[\"Final Exam Raw\"])} ({df.loc[\"Final Exam\"]} / {df.loc[\"Final Exam - Max Points\"]})')\n",
    "    gs_output['tests'].append({\n",
    "        'name': 'Final Exam',\n",
    "        'score': float(df['Final Exam Raw'] * 25),\n",
    "        'max_score': 30,\n",
    "        'output': '\\n'.join(output),\n",
    "        'visibility': 'after_published'\n",
    "    })\n",
    "\n",
    "# def output_disc_grades(df, gs_output):\n",
    "#     disc_score = df.loc['Discussion Raw']\n",
    "\n",
    "    \n",
    "#     di_weeks = eval(df['Discussion Weeks'])\n",
    "#     # print(di_weeks)\n",
    "#     # print(len(di_weeks))\n",
    "#     credit_received = [str(i + 1) for i in range(len(di_weeks)) if di_weeks[i] != 0]\n",
    "#     print(credit_received)\n",
    "\n",
    "#     # TODO Remove this because it only mattered for the first grade report\n",
    "#     attended = str(df.loc['Discussion Raw'])\n",
    "#     if attended == '0':\n",
    "#         disc_text = \"So far, you haven't attended any discussions or turned in any lab reflections.\\n\"\n",
    "#     else:\n",
    "#         # disc_text = \"\"\n",
    "#         disc_text = f\"Of the first {NUM_DI} discussions and currently graded lab reflections, you got credit for: Week(s) {', '.join(credit_received)}.\"\n",
    "\n",
    "#     # if not isinstance(df[\"Discussions Attended\"], str):\n",
    "#     #     disc_text = \"So far, you haven't attended any discussions or turned in any lab reflections.\\n\"\n",
    "#     # else:\n",
    "#     #     num_disc_attended = len(df[\"Discussions Attended\"].split(','))\n",
    "\n",
    "#     output = [f'Each week that you submit a lab, attended discussion, and submit the COMPLETE corresponding lab reflection form,\\nwe will add 0.2% of extra credit to your overall grade at the end of the quarter.\\n']\n",
    "#     output = np.append(output, disc_text)\n",
    "#     output = np.append(output, f\"This will add {round(0.2*disc_score, 2)}% extra credit to your total grade at the end of the quarter.\\n\")\n",
    "#     output = np.append(output, 'Since over 80% of the course filled out both SETs and the End of Quarter Survey, we added an extra 1% to your overall grade.')\n",
    "#     gs_output['tests'].append({\n",
    "#         'name': 'Extra Credits',\n",
    "#         'score': float(disc_score)*0.2 + float(OVERALL_EC) * 100,\n",
    "#         'max_score': 0,\n",
    "#         'output': '\\n'.join(output),\n",
    "#         'visibility': 'after_published'\n",
    "#     })\n",
    "\n",
    "# def output_ec_grades(df, gs_output):\n",
    "#     output = ['Since over 80% of the course filled out both SET and the End of Quarter Survey, everyone earned an extra 1%.']\n",
    "#     print(OVERALL_EC)\n",
    "#     print(type(OVERALL_EC))\n",
    "#     gs_output['tests'].append({\n",
    "#         'name': 'Extra Credit',\n",
    "#         'score': float(OVERALL_EC) * 100,\n",
    "#         'max score': 0,\n",
    "#         'output': '\\n'.join(output),\n",
    "#         'visibility': 'after_published'\n",
    "#     })\n",
    "\n",
    "gs_output = {'tests': []}\n",
    "output_letter_grade(df, gs_output)\n",
    "# output_slip_day_summary(df, gs_output)\n",
    "if NUM_LABS > 0:\n",
    "    output_lab_grades(df, gs_output)\n",
    "if NUM_PROJECTS > 0:\n",
    "    output_project_grades(df, gs_output)\n",
    "    output_project_checkpoints(df, gs_output)\n",
    "\n",
    "if YES_MIDTERM:\n",
    "    output_midterm_exam_grade(df, gs_output)\n",
    "\n",
    "if YES_FINAL:\n",
    "    output_final_exam_grade(df, gs_output)\n",
    "    \n",
    "# output_midterm_exam_grade(df, gs_output)\n",
    "# output_final_exam_grade(df, gs_output)\n",
    "\n",
    "# output_ec_grades(df, gs_output)\n",
    "\n",
    "# output_disc_grades(df, gs_output)\n",
    "\n",
    "\n",
    "\n",
    "# # Comment this cell out before exporting\n",
    "# for i in range(len(gs_output['tests'])):\n",
    "#     print(gs_output['tests'][i]['output'], '\\n')\n",
    "#     print('---')\n",
    "\n",
    "# out_path = '/autograder/results/results.json'\n",
    "# with open(out_path, 'w') as f:\n",
    "#     f.write(json.dumps(gs_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YES_MIDTERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdulrahim Ham\n",
      "amham@ucsd.edu\n",
      "A17531851\n",
      "last updated on 2024-05-30\n",
      "\n",
      "Your overall score in the course is 40.55%.\n",
      "Note that grades **will not** be rounded.\n",
      "Your current percentage based on your scores so far is 40.5498 / 57.3056 * 100 = 70.7606%.\n",
      "This accounts for dropped assignments – the lowest lab score was dropped\n",
      "\n",
      "Labs are worth 20% of your grade. There are 9 labs total.\n",
      "\n",
      "Lab 1: 0.9699 (129.0 / 133.0)\n",
      "Lab 2: 0.7736 (61.5 / 79.5) [Dropped]\n",
      "Lab 3: 0.9286 (52.0 / 56.0)\n",
      "Lab 4: 0.9624 (89.5 / 93.0)\n",
      "Lab 5: 0.9918 (60.5 / 61.0)\n",
      "Lab 6: 0.9107 (51.0 / 56.0)\n",
      "Lab 7: 0.9730 (108.0 / 111.0)\n",
      "Lab 8: 0 (not yet released or graded)\n",
      "Lab 9: 0 (not yet released or graded)\n",
      "Overall: 0.9300\n",
      "\n",
      "Projects are worth 25% of your grade. There are 4 projects. There are no drops. \n",
      "Projects 1, 2, and 3 are each worth 6% of your overall grade, and Project 4 is worth 12%, for a total of 30%.\n",
      "\n",
      "Project 1: 0.9268 (114.0 / 123.0)\n",
      "Project 2: 0.9109 (92.0 / 101.0)\n",
      "Project 3: 0.7900 (79.0 / 100.0)\n",
      "Project 4: 0 (not yet released or graded)\n",
      "Overall: 0.8759\n",
      "\n",
      "Project Checkpoints are worth 5% of your grade. There are 5 project checkpoints. There are no drops. \n",
      "The checkpoints for Projects 1, 2, and 3 are each worth 1% of your overall grade, and the checkpoint for Project 4 is worth 2%, for a total of 5%.\n",
      "\n",
      "Project 1 Checkpoint: 1.0000 (13.0 / 13.0)\n",
      "Project 2 Checkpoint: 1.0000 (5.0 / 5.0)\n",
      "Project 3 Checkpoint: 1.0000 (10.0 / 10.0)\n",
      "Project 4: 0 (not yet released or graded)\n",
      "Project 5: 0 (not yet released or graded)\n",
      "Overall: 1.0000\n",
      "\n",
      "The Midterm Exam is worth 20% of your grade.\n",
      "- Your score on the original Midterm Exam was 0.3600. \n",
      "    Since the original Midterm Exam had a mean of 0.5001 and SD of 0.1550,\n",
      "    your score as a z-score is -0.9039.\n",
      "- On the Final there will be questions to Redeem your Midterm Exam score. \n",
      "    If your Z-Score higher on these questions than your original Midterm Exam score, your Midterm Exam score will increase.\n",
      "    If you score lower, your Midterm Exam score will not change.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dict in gs_output[\"tests\"]:\n",
    "    print(dict[\"output\"]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
