{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "current_dateTime = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# sid = json.load(open(\"/autograder/submission/SID.json\", \"r\"))[\n",
    "#     \"SID\"\n",
    "# ]  # Needed for Gradescope\n",
    "\n",
    "# sys.path.append(\"${0%/*}\")\n",
    "\n",
    "sid = \"A17329510\"\n",
    "\n",
    "df = pd.read_csv(\"../data/grades_for_grade_report.csv\").set_index(\"SID\").loc[sid]\n",
    "\n",
    "stream = open(\"../configs.yaml\", \"r\")\n",
    "dictionary = yaml.safe_load(stream)\n",
    "\n",
    "# Data Loading Fields\n",
    "GRADES_FILENAME = dictionary[\"data_path\"][\"grades_filename\"]\n",
    "\n",
    "# Lab Fields\n",
    "NUM_LABS = dictionary[\"labs\"][\"num_labs\"]\n",
    "MAX_LABS = dictionary[\"labs\"][\"max_labs\"]\n",
    "\n",
    "# Project Fields\n",
    "NUM_PROJECTS = dictionary[\"projects\"][\"num_projects\"]\n",
    "MAX_PROJECTS = dictionary[\"projects\"][\"max_projects\"]\n",
    "\n",
    "NUM_PROJECT_CHECKPOINTS = dictionary[\"projects\"][\"num_checkpoints\"]\n",
    "MAX_PROJECT_CHECKPOINTS = dictionary[\"projects\"][\"max_checkpoints\"]\n",
    "\n",
    "# Midterm fields\n",
    "YES_MIDTERM = dictionary[\"exams\"][\"midterm\"][\"enabled\"]\n",
    "MIDTERM_VERSIONS = dictionary[\"exams\"][\"midterm\"][\"versions\"]\n",
    "MIDTERM_BONUS = dictionary[\"exams\"][\"midterm\"][\"bonus\"]\n",
    "MIDTERM_HAS_BONUS = MIDTERM_BONUS > 0\n",
    "\n",
    "# Final Fields\n",
    "YES_FINAL = dictionary[\"exams\"][\"final\"][\"enabled\"]\n",
    "FINAL_VERSIONS = dictionary[\"exams\"][\"final\"][\"versions\"]\n",
    "FINAL_BONUS = dictionary[\"exams\"][\"final\"][\"bonus\"]\n",
    "FINAL_HAS_BONUS = FINAL_BONUS > 0\n",
    "\n",
    "# Discussion Fields\n",
    "NUM_DISC_ATTENDENCE_REQUIRED = dictionary[\"discussions\"][\n",
    "    \"num_discussions_attendence_required\"\n",
    "]\n",
    "NUM_LECT_ATTENDENCE_REQUIRED = dictionary[\"discussions\"][\n",
    "    \"num_lecture_attendence_required\"\n",
    "]\n",
    "\n",
    "# Number of dropped assignments per category (set to 0 for no drops)\n",
    "# For example, if NUM_DROPS = 2, then 2 Labs and 2 Homeworks will be dropped\n",
    "# Note: Set this to 0 until the end of the quarter\n",
    "NUM_DROPS = dictionary[\"drop_policy\"][\"num_drops\"]\n",
    "\n",
    "OVERALL_EC = dictionary[\"extra_credit\"][\"overall\"]\n",
    "\n",
    "DROPS = NUM_DROPS > 0\n",
    "\n",
    "CURRENT_TOTAL_POSSIBLE_SCORE = df[\"Current Max Possible Score\"].max()\n",
    "\n",
    "ASSIGNMENT_WEIGHTS = dictionary[\"assignment_weights\"]\n",
    "# DISCUSSION_OR_LECTURE_ASSIGNMENT_WEIGHTS = dictionary[\n",
    "#     \"discussion_or_lecture_assignment_weights\"\n",
    "# ]\n",
    "# DISCUSSION_AND_LECTURE_ASSIGNMENT_WEIGHTS = dictionary[\n",
    "#     \"discussion_and_lecture_assignment_weights\"\n",
    "# ]\n",
    "\n",
    "DATE = current_dateTime\n",
    "\n",
    "DECIMAL_ROUND_PLACE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_round_str(n, round_place=DECIMAL_ROUND_PLACE):\n",
    "    rounded = str(np.round(n, round_place))\n",
    "    if len(rounded) < round_place + 2:\n",
    "        return str(rounded) + \"0\" * (round_place + 2 - len(rounded))\n",
    "    else:\n",
    "        return str(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_letter_grade(df, gs_output):\n",
    "    output = [f\"{df['Name']}\\n{df['Email']}\\n{sid}\\nlast updated on {DATE}\\n\"]\n",
    "    output = np.append(\n",
    "        output,\n",
    "        [\n",
    "            f\"Your overall score in the course is {np.round(df['Overall Score'], 3)}%.\\nNote that grades **will not** be rounded.\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        [\n",
    "            f\"Your current percentage based on your scores so far is {np.round(df['Overall Score'], DECIMAL_ROUND_PLACE)} / {np.round(CURRENT_TOTAL_POSSIBLE_SCORE, DECIMAL_ROUND_PLACE)} * 100 = {np.round(df['Current Score'], DECIMAL_ROUND_PLACE)}%.\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if not DROPS:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            [\n",
    "                f\"Also note that this grade report accounts for dropped assignments but no for extra credit yet.\"\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            [\n",
    "                \"This accounts for dropped assignments – the lowest lab score was dropped\"\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Overall Score\",\n",
    "            \"score\": 0,\n",
    "            \"max_score\": 0,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_discussion(df, gs_output):\n",
    "    discussion_weight = df[\"discussion_offset\"] + df[\"lecture_offset\"]\n",
    "\n",
    "    discussion_attended = df[\"disc_count\"]\n",
    "    lecture_attended = df[\"lecture_count\"]\n",
    "\n",
    "    output = [\n",
    "        f\"Students who attended {NUM_DISC_ATTENDENCE_REQUIRED} discussions will receive full discussion credit, worth 5% of your overall grade.\\nStudents who attended  {NUM_LECT_ATTENDENCE_REQUIRED} lectures will will receive full discussion credit, worth 5% of your overall grade.\\nIf you attend fewer discussions or lectures, your midterm and final exam grades will be adjusted accordingly, increasing their weight by the proportion of unattended sessions.\\n\"\n",
    "    ]\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f\"You attended {discussion_attended} discussions and {lecture_attended} lectures.\",\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Your grade with discussion and lecture credit is {even_round_str(df[\"Overall Score with Discussion and Lecture\"])}',\n",
    "    )\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Your grade with just discussion credit is {even_round_str(df[\"Overall Score with Discussion\"])}',\n",
    "    )\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Your grade with just lecture credit is {even_round_str(df[\"Overall Score with Lecture\"])}',\n",
    "    )\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Your grade without discussion or lecture is {even_round_str(df[\"Overall Score without Discussion or Lecture\"])}',\n",
    "    )\n",
    "\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Discussion and Lecture Attendance\",\n",
    "            \"score\": float(discussion_weight),\n",
    "            \"max_score\": discussion_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_assignment_string(assignment):\n",
    "    kind = assignment.split(\" \")[0]\n",
    "    exists = df.loc[assignment + \" - Max Points\"] != 0\n",
    "    if exists:\n",
    "        output = f'{assignment}: {even_round_str(df.loc[assignment + \" Final Grade\"])}'\n",
    "        output += f' ({even_round_str(df.loc[assignment + \" Final Grade\"] * df.loc[assignment + \" - Max Points\"])} / {df.loc[assignment + \" - Max Points\"]})'\n",
    "        if DROPS and \"Lab\" in assignment:\n",
    "            if assignment in df[f\"Dropped {kind}s\"]:\n",
    "                output += \" [Dropped]\"\n",
    "    else:\n",
    "        output = f\"{assignment}: 0 (Not yet released or graded)\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lab_grades(df, gs_output):\n",
    "    lab_score = df[\"Lab Average\"]\n",
    "    lab_weight = int(ASSIGNMENT_WEIGHTS[\"lab\"] * 100)\n",
    "    output = [\n",
    "        f\"Labs are worth {lab_weight}% of your grade. There are {MAX_LABS} labs total.\\n\"\n",
    "    ]\n",
    "    for lab in range(1, MAX_LABS + 1):\n",
    "        if lab <= NUM_LABS:\n",
    "            output = np.append(output, make_assignment_string(f\"Lab {lab}\"))\n",
    "        else:\n",
    "            output = np.append(output, f\"Lab {lab}: 0 (not yet released or graded)\")\n",
    "    output = np.append(output, [f\"Overall: {even_round_str(df.loc['Lab Average'])}\"])\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Labs\",\n",
    "            \"score\": float(lab_score) * lab_weight,\n",
    "            \"max_score\": lab_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_project_grades(df, gs_output):\n",
    "    project_score = df.loc[\"Project Average\"]\n",
    "    project_weight = int(ASSIGNMENT_WEIGHTS[\"project\"] * 100)\n",
    "    each_project_weight = int(project_weight / 5)\n",
    "    output = [\n",
    "        f\"Projects are worth {project_weight}% of your grade. There are {MAX_PROJECTS} projects. There are no drops. \\nProjects 1, 2, and 3 are each worth {each_project_weight}% of your overall grade, and Project 4 is worth {2*each_project_weight}%, for a total of {project_weight}%.\\n\"\n",
    "    ]\n",
    "    for project in range(1, MAX_PROJECTS + 1):\n",
    "        # if project == 4:\n",
    "        #     project_str = '4A'\n",
    "        # elif project == 5:\n",
    "        #     project_str = '4B'\n",
    "        # else:\n",
    "        project_str = str(project)\n",
    "        if project <= NUM_PROJECTS:\n",
    "\n",
    "            output = np.append(output, make_assignment_string(f\"Project {project_str}\"))\n",
    "        else:\n",
    "            output = np.append(\n",
    "                output, f\"Project {project_str}: 0 (not yet released or graded)\"\n",
    "            )\n",
    "\n",
    "    output = np.append(\n",
    "        output, [f\"Overall: {even_round_str(df.loc['Project Average'])}\"]\n",
    "    )\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Projects\",\n",
    "            \"score\": float(project_score) * project_weight,\n",
    "            \"max_score\": project_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_project_checkpoints(df, gs_output):\n",
    "    checkpoint_score = df.loc[\"Project Checkpoint Average\"]\n",
    "    checkpoint_weight = int(ASSIGNMENT_WEIGHTS[\"project_checkpoint\"] * 100)\n",
    "    output = [\n",
    "        f\"Project Checkpoints are worth {checkpoint_weight}% of your grade. There are {MAX_PROJECT_CHECKPOINTS} project checkpoints. There are no drops. \\nEach checkpoints are each worth 1% of your overall grade.\\n\"\n",
    "    ]\n",
    "    for checkpoint in range(1, MAX_PROJECT_CHECKPOINTS + 1):\n",
    "        # if checkpoint == 4:\n",
    "        #     checkpoint_str = '4.1'\n",
    "        # elif checkpoint == 5:\n",
    "        #     checkpoint_str = '4.2'\n",
    "        # else:\n",
    "        checkpoint_str = str(checkpoint)\n",
    "        if checkpoint <= NUM_PROJECT_CHECKPOINTS:\n",
    "            output = np.append(\n",
    "                output, make_assignment_string(f\"Project {checkpoint_str} Checkpoint\")\n",
    "            )\n",
    "        else:\n",
    "            output = np.append(\n",
    "                output, f\"Project {checkpoint_str}: 0 (not yet released or graded)\"\n",
    "            )\n",
    "\n",
    "    output = np.append(\n",
    "        output, [f\"Overall: {even_round_str(df.loc['Project Checkpoint Average'])}\"]\n",
    "    )\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Project Checkpoints\",\n",
    "            \"score\": float(checkpoint_score) * checkpoint_weight,\n",
    "            \"max_score\": checkpoint_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_midterm_exam_grade(df, gs_output):\n",
    "    # if df[\"Used Discussion and Lecture\"]:\n",
    "    #     midterm_weight = float(\n",
    "    #         DISCUSSION_AND_LECTURE_ASSIGNMENT_WEIGHTS[\"midterm_exam\"] * 100\n",
    "    #     )\n",
    "    # elif df[\"Used Discussion or Lecture\"]:\n",
    "    #     midterm_weight = float(\n",
    "    #         DISCUSSION_OR_LECTURE_ASSIGNMENT_WEIGHTS[\"midterm_exam\"] * 100\n",
    "    #     )\n",
    "    # else:\n",
    "    midterm_weight = float(\n",
    "        (\n",
    "            ASSIGNMENT_WEIGHTS[\"midterm_exam\"]\n",
    "            - (df[\"discussion_offset\"] + df[\"lecture_offset\"]) / 2\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    midterm_score = df.loc[\"Midterm Average\"]\n",
    "\n",
    "    output = [f\"The Midterm Exam is worth {midterm_weight:.2f}% of your grade.\"]\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Pre-Redemption Score: {even_round_str(df.loc[\"Midterm Exam Grade\"])} ({df.loc[\"Midterm\"]} / {df.loc[\"Midterm - Max Points\"]})',\n",
    "    )\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Mean: {even_round_str(df.loc[\"Midterm Mean\"])}; Midterm std: {even_round_str(df.loc[\"Midterm std\"])}',\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm z-score: {even_round_str(df.loc[\"Midterm Z-Score\"])} = ({even_round_str(df.loc[\"Midterm Exam Grade\"])} - {even_round_str(df.loc[\"Midterm Mean\"])}) / {even_round_str(df.loc[\"Midterm std\"])}',\n",
    "    )\n",
    "    # if df['Midterm Clobbered']:\n",
    "    #     output = np.append(output, f\"Since you made a special arrangement to replace your Midterm Exam score with your Final Exam score, the score you see above is the same in standard units as your Final Exam score.\")\n",
    "\n",
    "    output = np.append(\n",
    "        output, f'The redemption questions on the final exam are marked with \"(M)\".'\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Redemption Mean: {even_round_str(df.loc[\"Midterm Redemption Mean\"])}; Midterm Redemption std: {even_round_str(df.loc[\"Midterm Redemption std\"])}',\n",
    "    )\n",
    "\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Midterm Redemption z-score: {even_round_str(df.loc[\"Midterm Redemption Z-Score\"])} = ({even_round_str(df.loc[\"Redemption Score\"])} - {even_round_str(df.loc[\"Midterm Redemption Mean\"])}) / {even_round_str(df.loc[\"Midterm Redemption std\"])}',\n",
    "    )\n",
    "\n",
    "    if df[\"Redemption Successful\"]:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            f\"Redemption Successful: new midterm score is {even_round_str(midterm_score)} = {even_round_str(df['Midterm Redemption Z-Score'])} * {even_round_str(df.loc['Midterm std'])} + {even_round_str(df.loc['Midterm Mean'])}.\",\n",
    "        )\n",
    "    else:\n",
    "        output = np.append(\n",
    "            output,\n",
    "            f\"Redemption Unsuccessful: your new z-score is not higher than your old z-score.\",\n",
    "        )\n",
    "\n",
    "    output = np.append(output, f\"Midterm Final Score: {even_round_str(midterm_score)}\")\n",
    "\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Midterm Exam\",\n",
    "            \"score\": float(midterm_score * midterm_weight),\n",
    "            \"max_score\": midterm_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Midterm Exam Grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_final_exam_grade(df, gs_output):\n",
    "    # if df[\"Used Discussion and Lecture\"]:\n",
    "    #     final_exam_weight = float(\n",
    "    #         DISCUSSION_AND_LECTURE_ASSIGNMENT_WEIGHTS[\"final_exam\"] * 100\n",
    "    #     )\n",
    "    # elif df[\"Used Discussion or Lecture\"]:\n",
    "    #     final_exam_weight = float(\n",
    "    #         DISCUSSION_OR_LECTURE_ASSIGNMENT_WEIGHTS[\"final_exam\"] * 100\n",
    "    #     )\n",
    "    # else:\n",
    "    final_exam_weight = (\n",
    "        float(\n",
    "            ASSIGNMENT_WEIGHTS[\"final_exam\"]\n",
    "            - (df[\"discussion_offset\"] + df[\"lecture_offset\"]) / 2\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    final_exam_score = df.loc[\"Final Average\"]\n",
    "\n",
    "    output = [f\"The Final Exam is worth {final_exam_weight:.2f}% of your grade.\"]\n",
    "    output = np.append(\n",
    "        output,\n",
    "        f'Score: {even_round_str(final_exam_score)} ({df.loc[\"Final Exam\"]} / {df.loc[\"Final Exam - Max Points\"]})',\n",
    "    )\n",
    "    gs_output[\"tests\"].append(\n",
    "        {\n",
    "            \"name\": \"Final Exam\",\n",
    "            \"score\": float(final_exam_score * final_exam_weight),\n",
    "            \"max_score\": final_exam_weight,\n",
    "            \"output\": \"\\n\".join(output),\n",
    "            \"visibility\": \"after_published\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_output = {\"tests\": []}\n",
    "output_letter_grade(df, gs_output)\n",
    "output_discussion(df, gs_output)\n",
    "if NUM_LABS > 0:\n",
    "    output_lab_grades(df, gs_output)\n",
    "if NUM_PROJECTS > 0:\n",
    "    output_project_grades(df, gs_output)\n",
    "    output_project_checkpoints(df, gs_output)\n",
    "\n",
    "if YES_MIDTERM:\n",
    "    output_midterm_exam_grade(df, gs_output)\n",
    "\n",
    "if YES_FINAL:\n",
    "    output_final_exam_grade(df, gs_output)\n",
    "\n",
    "\n",
    "# out_path = \"/autograder/results/results.json\"\n",
    "# with open(out_path, \"w\") as f:\n",
    "#     f.write(json.dumps(gs_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul Sengupta\n",
      "rasengupta@ucsd.edu\n",
      "A17329510\n",
      "last updated on 2024-06-18\n",
      "\n",
      "Your overall score in the course is 80.794%.\n",
      "Note that grades **will not** be rounded.\n",
      "Your current percentage based on your scores so far is 80.794 / 100.0 * 100 = 80.794%.\n",
      "This accounts for dropped assignments – the lowest lab score was dropped\n",
      "\n",
      "Students who attended 7 discussions will receive full discussion credit, worth 5% of your overall grade.\n",
      "Students who attended  14 lectures will will receive full discussion credit, worth 5% of your overall grade.\n",
      "If you attend fewer discussions or lectures, your midterm and final exam grades will be adjusted accordingly, increasing their weight by the proportion of unattended sessions.\n",
      "\n",
      "You attended 8.0 discussions and 5.0 lectures.\n",
      "Your grade with discussion and lecture credit is 80.794\n",
      "Your grade with just discussion credit is 80.176\n",
      "Your grade with just lecture credit is 79.064\n",
      "Your grade without discussion or lecture is 78.446\n",
      "\n",
      "Labs are worth 20% of your grade. There are 9 labs total.\n",
      "\n",
      "Lab 1: 0.925 (123.0 / 133.0)\n",
      "Lab 2: 0.936 (74.40 / 79.5)\n",
      "Lab 3: 0.936 (52.40 / 56.0)\n",
      "Lab 4: 0.952 (88.50 / 93.0)\n",
      "Lab 5: 1.000 (61.00 / 61.0)\n",
      "Lab 6: 0.893 (50.00 / 56.0)\n",
      "Lab 7: 0.957 (106.2 / 111.0)\n",
      "Lab 8: 0.988 (64.20 / 65.0)\n",
      "Lab 9: 0.553 (30.40 / 55.0) [Dropped]\n",
      "Overall: 0.948\n",
      "\n",
      "Projects are worth 25% of your grade. There are 4 projects. There are no drops. \n",
      "Projects 1, 2, and 3 are each worth 5% of your overall grade, and Project 4 is worth 10%, for a total of 25%.\n",
      "\n",
      "Project 1: 0.813 (100.0 / 123.0)\n",
      "Project 2: 0.861 (87.00 / 101.0)\n",
      "Project 3: 0.860 (86.00 / 100.0)\n",
      "Project 4: 0.965 (193.0 / 200.0)\n",
      "Overall: 0.893\n",
      "\n",
      "Project Checkpoints are worth 5% of your grade. There are 5 project checkpoints. There are no drops. \n",
      "Each checkpoints are each worth 1% of your overall grade.\n",
      "\n",
      "Project 1 Checkpoint: 1.000 (13.00 / 13.0)\n",
      "Project 2 Checkpoint: 1.000 (5.000 / 5.0)\n",
      "Project 3 Checkpoint: 1.000 (10.00 / 10.0)\n",
      "Project 4 Checkpoint: 0.600 (12.00 / 20.0)\n",
      "Project 5 Checkpoint: 1.000 (20.00 / 20.0)\n",
      "Overall: 0.920\n",
      "\n",
      "The Midterm Exam is worth 16.61% of your grade.\n",
      "Midterm Pre-Redemption Score: 0.300 (15.0 / 50.0)\n",
      "Midterm Mean: 0.499; Midterm std: 0.155\n",
      "Midterm z-score: -1.288 = (0.300 - 0.499) / 0.155\n",
      "The redemption questions on the final exam are marked with \"(M)\".\n",
      "Midterm Redemption Mean: 21.38; Midterm Redemption std: 4.244\n",
      "Midterm Redemption z-score: 1.088 = (26.00 - 21.38) / 4.244\n",
      "Redemption Successful: new midterm score is 0.668 = 1.088 * 0.155 + 0.499.\n",
      "Midterm Final Score: 0.668\n",
      "\n",
      "The Final Exam is worth 26.61% of your grade.\n",
      "Score: 0.640 (64.0 / 100.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dict in gs_output[\"tests\"]:\n",
    "    print(dict[\"output\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
